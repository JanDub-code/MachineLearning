#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
F√ÅZE 3: Tr√©nov√°n√≠ AI Modelu pro Predikci Fundament≈Ø
====================================================

Tento skript tr√©nuje Random Forest model, kter√Ω se nauƒç√≠ predikovat
fundament√°ln√≠ metriky z OHLCV dat a technick√Ωch indik√°tor≈Ø.

Input: OHLCV + technick√© indik√°tory (2024-2025)
Output: 15 fundament√°ln√≠ch metrik (P/E, ROE, atd.)

Model: Multi-output Random Forest Regressor

V√Ωstup: models/fundamental_predictor.pkl
"""

import os
import sys
import time
import warnings
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from joblib import dump, load
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# === KONFIGURACE ===
OHLCV_DIR = "../data_10y"
FUNDAMENTALS_DIR = "../data/fundamentals"
OUTPUT_DIR = "../models"
ANALYSIS_DIR = "../data/analysis"

# Features z OHLCV dat
OHLCV_FEATURES = [
    'open', 'high', 'low', 'close', 'volume',
    'volatility', 'returns',
    'rsi_14', 'macd', 'macd_signal', 'macd_hist',
    'sma_3', 'sma_6', 'sma_12',
    'ema_3', 'ema_6', 'ema_12',
    'volume_change'
]

# Target fundament√°ln√≠ metriky
FUNDAMENTAL_TARGETS = [
    'PE', 'PB', 'PS', 'EV_EBITDA',  # Valuaƒçn√≠
    'ROE', 'ROA', 'Profit_Margin', 'Operating_Margin', 'Gross_Margin',  # Profitabilita
    'Debt_to_Equity', 'Current_Ratio', 'Quick_Ratio',  # Finanƒçn√≠ zdrav√≠
    'Revenue_Growth_YoY', 'Earnings_Growth_YoY'  # R≈Øst
]

# Hyperparametry Random Forest
RF_PARAMS = {
    'n_estimators': 100,
    'max_depth': 20,
    'min_samples_split': 5,
    'min_samples_leaf': 2,
    'random_state': 42,
    'n_jobs': -1,
    'verbose': 0
}

def log(msg: str):
    """Logov√°n√≠ s ƒçasovou znaƒçkou"""
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def ensure_dir(path: str):
    """Vytvo≈ô√≠ slo≈æku pokud neexistuje"""
    os.makedirs(path, exist_ok=True)

def load_ohlcv_data() -> pd.DataFrame:
    """Naƒçte OHLCV data s technick√Ωmi indik√°tory (2015-2025)"""
    log("üìÇ Naƒç√≠t√°m OHLCV data...")
    
    ohlcv_path = os.path.join(OHLCV_DIR, "all_sectors_full_10y.csv")
    
    if not os.path.exists(ohlcv_path):
        log(f"‚ùå Soubor nenalezen: {ohlcv_path}")
        sys.exit(1)
    
    df = pd.read_csv(ohlcv_path)
    df['date'] = pd.to_datetime(df['date'])
    
    log(f"‚úì Naƒçteno {len(df)} z√°znam≈Ø ({df['date'].min()} ‚Üí {df['date'].max()})")
    
    return df

def load_fundamentals() -> pd.DataFrame:
    """Naƒçte fundament√°ln√≠ data (2024-2025)"""
    log("üìÇ Naƒç√≠t√°m fundament√°ln√≠ data...")
    
    fund_path = os.path.join(FUNDAMENTALS_DIR, "all_sectors_fundamentals.csv")
    
    if not os.path.exists(fund_path):
        log(f"‚ùå Soubor nenalezen: {fund_path}")
        log("   Nejprve spus≈•te: python scripts/1_download_fundamentals.py")
        sys.exit(1)
    
    df = pd.read_csv(fund_path)
    df['date'] = pd.to_datetime(df['date'])
    
    log(f"‚úì Naƒçteno {len(df)} z√°znam≈Ø ({df['date'].min()} ‚Üí {df['date'].max()})")
    
    return df

def merge_ohlcv_fundamentals(ohlcv: pd.DataFrame, fundamentals: pd.DataFrame) -> pd.DataFrame:
    """
    Spoj√≠ OHLCV data s fundament√°ln√≠mi daty.
    Pou≈æ√≠v√° forward-fill pro fundamenty (quarterly ‚Üí monthly).
    """
    log("üîó Spojuji OHLCV a fundament√°ln√≠ data...")
    
    # P≈ôev√©st fundamenty na mƒõs√≠ƒçn√≠ frekvenci (forward-fill)
    fundamentals = fundamentals.sort_values('date')
    
    # Pro ka≈æd√Ω ticker zvl√°≈°≈•
    merged_parts = []
    
    for ticker in fundamentals['ticker'].unique():
        # OHLCV pro ticker
        ohlcv_ticker = ohlcv[ohlcv['ticker'] == ticker].copy()
        ohlcv_ticker = ohlcv_ticker.sort_values('date').set_index('date')
        
        # Fundamenty pro ticker
        fund_ticker = fundamentals[fundamentals['ticker'] == ticker].copy()
        fund_ticker = fund_ticker.sort_values('date').set_index('date')
        
        # Merge s forward-fill
        merged = ohlcv_ticker.join(fund_ticker[FUNDAMENTAL_TARGETS], how='left')
        merged[FUNDAMENTAL_TARGETS] = merged[FUNDAMENTAL_TARGETS].fillna(method='ffill')
        
        merged = merged.reset_index()
        merged_parts.append(merged)
    
    result = pd.concat(merged_parts, ignore_index=True)
    
    # Filtrovat pouze obdob√≠ kde m√°me fundamenty (2024-2025)
    result = result[result['date'] >= '2024-01-01'].copy()
    
    # Odstranit ≈ô√°dky s chybƒõj√≠c√≠mi daty
    result = result.dropna(subset=OHLCV_FEATURES + FUNDAMENTAL_TARGETS)
    
    log(f"‚úì Spojeno: {len(result)} z√°znam≈Ø")
    log(f"  ‚Ä¢ Tickery: {result['ticker'].nunique()}")
    log(f"  ‚Ä¢ Obdob√≠: {result['date'].min()} ‚Üí {result['date'].max()}")
    
    return result

def prepare_training_data(df: pd.DataFrame):
    """
    P≈ôiprav√≠ data pro tr√©nov√°n√≠.
    Vrac√≠: X_train, X_test, y_train, y_test, scaler
    """
    log("üîß P≈ô√≠prava tr√©novac√≠ch dat...")
    
    # Features a targets
    X = df[OHLCV_FEATURES].copy()
    y = df[FUNDAMENTAL_TARGETS].copy()
    
    # Odstranit nekoneƒçn√© hodnoty
    X = X.replace([np.inf, -np.inf], np.nan)
    y = y.replace([np.inf, -np.inf], np.nan)
    
    # Dropnout NaN
    valid_mask = ~(X.isna().any(axis=1) | y.isna().any(axis=1))
    X = X[valid_mask]
    y = y[valid_mask]
    
    log(f"‚úì Validn√≠ch vzork≈Ø: {len(X)}")
    
    # Train/test split (80/20)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )
    
    log(f"  ‚Ä¢ Train: {len(X_train)} vzork≈Ø")
    log(f"  ‚Ä¢ Test: {len(X_test)} vzork≈Ø")
    
    # Standardizace features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train.values, y_test.values, scaler

def train_random_forest(X_train, y_train):
    """Tr√©nuje Multi-output Random Forest model"""
    log("ü§ñ Tr√©nov√°n√≠ Random Forest modelu...")
    log(f"  ‚Ä¢ Parametry: {RF_PARAMS}")
    
    start_time = time.time()
    
    # Multi-output Random Forest
    model = MultiOutputRegressor(
        RandomForestRegressor(**RF_PARAMS)
    )
    
    model.fit(X_train, y_train)
    
    elapsed = time.time() - start_time
    log(f"‚úì Tr√©nov√°n√≠ dokonƒçeno za {elapsed:.1f}s")
    
    return model

def evaluate_model(model, X_test, y_test, feature_names, target_names):
    """Evaluace modelu na testovac√≠ch datech"""
    log("\nüìä Evaluace modelu...")
    
    # Predikce
    y_pred = model.predict(X_test)
    
    # Metriky pro ka≈æd√Ω target
    results = []
    
    for i, target in enumerate(target_names):
        y_true_i = y_test[:, i]
        y_pred_i = y_pred[:, i]
        
        mae = mean_absolute_error(y_true_i, y_pred_i)
        rmse = np.sqrt(mean_squared_error(y_true_i, y_pred_i))
        r2 = r2_score(y_true_i, y_pred_i)
        
        # Relativn√≠ MAE (%)
        mean_val = np.abs(y_true_i).mean()
        mae_pct = (mae / mean_val * 100) if mean_val > 0 else np.nan
        
        results.append({
            'target': target,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'mae_pct': mae_pct
        })
        
        log(f"  ‚Ä¢ {target:20s}: MAE={mae:8.3f} ({mae_pct:5.1f}%)  RMSE={rmse:8.3f}  R¬≤={r2:6.3f}")
    
    results_df = pd.DataFrame(results)
    
    # Celkov√Ω pr≈Ømƒõr
    log(f"\n  üìà PR≈ÆMƒöR:")
    log(f"     MAE: {results_df['mae'].mean():.3f}")
    log(f"     MAE%: {results_df['mae_pct'].mean():.1f}%")
    log(f"     RMSE: {results_df['rmse'].mean():.3f}")
    log(f"     R¬≤: {results_df['r2'].mean():.3f}")
    
    return results_df, y_pred

def extract_feature_importance(model, feature_names, target_names):
    """
    Extrahuje feature importance pro ka≈æd√Ω target.
    """
    log("\nüîç Anal√Ωza Feature Importance...")
    
    importance_data = []
    
    for i, estimator in enumerate(model.estimators_):
        target = target_names[i]
        importances = estimator.feature_importances_
        
        for j, feature in enumerate(feature_names):
            importance_data.append({
                'target': target,
                'feature': feature,
                'importance': importances[j]
            })
    
    importance_df = pd.DataFrame(importance_data)
    
    # Top 5 features pro ka≈æd√Ω target
    for target in target_names:
        target_imp = importance_df[importance_df['target'] == target].sort_values('importance', ascending=False)
        top5 = target_imp.head(5)
        
        log(f"\n  {target}:")
        for _, row in top5.iterrows():
            log(f"    ‚Ä¢ {row['feature']:15s}: {row['importance']:.4f}")
    
    return importance_df

def save_results(model, scaler, metrics_df, importance_df, y_test, y_pred, target_names):
    """Ulo≈æen√≠ modelu, metrik a anal√Ωz"""
    log("\nüíæ Ukl√°d√°m v√Ωsledky...")
    
    ensure_dir(OUTPUT_DIR)
    ensure_dir(ANALYSIS_DIR)
    
    # 1. Model a scaler
    model_path = os.path.join(OUTPUT_DIR, "fundamental_predictor.pkl")
    scaler_path = os.path.join(OUTPUT_DIR, "feature_scaler.pkl")
    
    dump(model, model_path)
    dump(scaler, scaler_path)
    
    log(f"‚úì Model: {model_path}")
    log(f"‚úì Scaler: {scaler_path}")
    
    # 2. Metriky
    metrics_path = os.path.join(ANALYSIS_DIR, "fundamental_predictor_metrics.csv")
    metrics_df.to_csv(metrics_path, index=False)
    log(f"‚úì Metriky: {metrics_path}")
    
    # 3. Feature importance
    importance_path = os.path.join(ANALYSIS_DIR, "feature_importance_fundamentals.csv")
    importance_df.to_csv(importance_path, index=False)
    log(f"‚úì Feature Importance: {importance_path}")
    
    # 4. Predictions vs Actual
    pred_df = pd.DataFrame(y_test, columns=[f"{t}_true" for t in target_names])
    pred_df_pred = pd.DataFrame(y_pred, columns=[f"{t}_pred" for t in target_names])
    pred_df = pd.concat([pred_df, pred_df_pred], axis=1)
    
    pred_path = os.path.join(ANALYSIS_DIR, "fundamental_predictions_vs_actual.csv")
    pred_df.to_csv(pred_path, index=False)
    log(f"‚úì Predictions: {pred_path}")

def main():
    log("="*80)
    log("F√ÅZE 3: TR√âNOV√ÅN√ç AI MODELU PRO PREDIKCI FUNDAMENT≈Æ")
    log("="*80)
    
    start_time = time.time()
    
    # 1. Naƒçten√≠ dat
    ohlcv = load_ohlcv_data()
    fundamentals = load_fundamentals()
    
    # 2. Spojen√≠ dat
    merged = merge_ohlcv_fundamentals(ohlcv, fundamentals)
    
    # 3. P≈ô√≠prava tr√©novac√≠ch dat
    X_train, X_test, y_train, y_test, scaler = prepare_training_data(merged)
    
    # 4. Tr√©nov√°n√≠ modelu
    model = train_random_forest(X_train, y_train)
    
    # 5. Evaluace
    metrics_df, y_pred = evaluate_model(model, X_test, y_test, OHLCV_FEATURES, FUNDAMENTAL_TARGETS)
    
    # 6. Feature importance
    importance_df = extract_feature_importance(model, OHLCV_FEATURES, FUNDAMENTAL_TARGETS)
    
    # 7. Ulo≈æen√≠ v√Ωsledk≈Ø
    save_results(model, scaler, metrics_df, importance_df, y_test, y_pred, FUNDAMENTAL_TARGETS)
    
    elapsed = time.time() - start_time
    
    log("\n" + "="*80)
    log("‚úÖ HOTOVO!")
    log("="*80)
    log(f"‚è±  Celkov√Ω ƒças: {elapsed/60:.1f} minut")
    log(f"üéØ Pr≈Ømƒõrn√° p≈ôesnost: {metrics_df['mae_pct'].mean():.1f}% MAE")
    log(f"üìä R¬≤ score: {metrics_df['r2'].mean():.3f}")
    
    # Doporuƒçen√≠
    avg_mae_pct = metrics_df['mae_pct'].mean()
    if avg_mae_pct < 15:
        log("\n‚ú® V√Ωbornƒõ! Model dos√°hl c√≠lov√© p≈ôesnosti (<15% MAE)")
    elif avg_mae_pct < 20:
        log("\nüëç Dob≈ôe! Model je pou≈æiteln√Ω (15-20% MAE)")
    else:
        log("\n‚ö†Ô∏è  Model m√° vy≈°≈°√≠ chybu (>20% MAE). Zva≈æte:")
        log("   ‚Ä¢ V√≠ce dat (del≈°√≠ obdob√≠)")
        log("   ‚Ä¢ Hyperparameter tuning")
        log("   ‚Ä¢ Feature engineering")
    
    log("\n" + "="*80)
    log("Dal≈°√≠ krok: python scripts/3_complete_historical_data.py")
    log("="*80)

if __name__ == "__main__":
    main()
